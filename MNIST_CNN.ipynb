{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m2JWFliFfKT"
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from torchvision import datasets, transforms\n",
        "from torchsummary import summary\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_Cx9q2QFgM7"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # Input Block - Enhanced feature extraction\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, 3, padding=1),    # 28x28x8\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.Conv2d(8, 8, 3, padding=1),    # 28x28x8\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.Conv2d(8, 16, 3, padding=1),   # 28x28x16\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(0.01)\n",
        "        )\n",
        "\n",
        "        # Transition Block 1\n",
        "        self.trans1 = nn.Sequential(\n",
        "            nn.MaxPool2d(2, 2),              # 14x14x16\n",
        "        )\n",
        "\n",
        "        # Convolution Block 2 - Focus on distinguishing similar digits\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 16, 3, padding=1),  # 14x14x16\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Conv2d(16, 32, 3, padding=1),  # 14x14x32\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(0.01)\n",
        "        )\n",
        "\n",
        "        # Transition Block 2\n",
        "        self.trans2 = nn.Sequential(\n",
        "            nn.MaxPool2d(2, 2),              # 7x7x32\n",
        "        )\n",
        "\n",
        "        # Convolution Block 3 - Final feature refinement\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(32, 32, 3, padding=1),  # 7x7x32\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, 16, 1),            # 7x7x16 (pointwise)\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(0.01)\n",
        "        )\n",
        "\n",
        "        self.gap = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=7)      # 1x1x16\n",
        "        )\n",
        "\n",
        "        self.final = nn.Sequential(\n",
        "            nn.Conv2d(16, 10, 1)             # 1x1x10\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.trans1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.trans2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.gap(x)\n",
        "        x = self.final(x)\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdydjYTZFyi3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7a28b64-bc68-4d12-9b03-28a9ffc75540"
      },
      "source": [
        "model = Net().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 8, 28, 28]              80\n",
            "              ReLU-2            [-1, 8, 28, 28]               0\n",
            "       BatchNorm2d-3            [-1, 8, 28, 28]              16\n",
            "            Conv2d-4            [-1, 8, 28, 28]             584\n",
            "              ReLU-5            [-1, 8, 28, 28]               0\n",
            "       BatchNorm2d-6            [-1, 8, 28, 28]              16\n",
            "            Conv2d-7           [-1, 16, 28, 28]           1,168\n",
            "              ReLU-8           [-1, 16, 28, 28]               0\n",
            "       BatchNorm2d-9           [-1, 16, 28, 28]              32\n",
            "          Dropout-10           [-1, 16, 28, 28]               0\n",
            "        MaxPool2d-11           [-1, 16, 14, 14]               0\n",
            "           Conv2d-12           [-1, 16, 14, 14]           2,320\n",
            "             ReLU-13           [-1, 16, 14, 14]               0\n",
            "      BatchNorm2d-14           [-1, 16, 14, 14]              32\n",
            "           Conv2d-15           [-1, 32, 14, 14]           4,640\n",
            "             ReLU-16           [-1, 32, 14, 14]               0\n",
            "      BatchNorm2d-17           [-1, 32, 14, 14]              64\n",
            "          Dropout-18           [-1, 32, 14, 14]               0\n",
            "        MaxPool2d-19             [-1, 32, 7, 7]               0\n",
            "           Conv2d-20             [-1, 32, 7, 7]           9,248\n",
            "             ReLU-21             [-1, 32, 7, 7]               0\n",
            "      BatchNorm2d-22             [-1, 32, 7, 7]              64\n",
            "           Conv2d-23             [-1, 16, 7, 7]             528\n",
            "             ReLU-24             [-1, 16, 7, 7]               0\n",
            "      BatchNorm2d-25             [-1, 16, 7, 7]              32\n",
            "          Dropout-26             [-1, 16, 7, 7]               0\n",
            "        AvgPool2d-27             [-1, 16, 1, 1]               0\n",
            "           Conv2d-28             [-1, 10, 1, 1]             170\n",
            "================================================================\n",
            "Total params: 18,994\n",
            "Trainable params: 18,994\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.03\n",
            "Params size (MB): 0.07\n",
            "Estimated Total Size (MB): 1.10\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqTWLaM5GHgH"
      },
      "source": [
        "torch.manual_seed(1)\n",
        "batch_size = 156\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomRotation((-3.0, 3.0), fill=(1,)),  # Very conservative rotation\n",
        "    transforms.RandomAffine(\n",
        "        degrees=0,\n",
        "        translate=(0.05, 0.05),  # Reduced translation\n",
        "        scale=(0.98, 1.02),     # Minimal scaling\n",
        "        shear=(-2, 2),          # Minimal shear\n",
        "        fill=(1,)\n",
        "    ),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "kwargs = {'num_workers': 2, 'pin_memory': True} if use_cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                  transform=train_transforms),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=test_transforms),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    processed = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        processed += len(data)\n",
        "\n",
        "        pbar.set_description(desc=f'Loss={loss.item():0.4f} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}%')\n",
        "\n",
        "def test_with_misclassified(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    misclassified_images = []\n",
        "    misclassified_pred = []\n",
        "    misclassified_target = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "\n",
        "            # Store misclassified images\n",
        "            misclassified_mask = ~pred.eq(target.view_as(pred)).squeeze()\n",
        "            if misclassified_mask.any():\n",
        "                misclassified_imgs = data[misclassified_mask].cpu()\n",
        "                pred_np = pred[misclassified_mask].cpu().numpy()\n",
        "                target_np = target[misclassified_mask].cpu().numpy()\n",
        "\n",
        "                # Handle both single and multiple misclassifications\n",
        "                if len(misclassified_mask.size()) == 0:  # Single misclassification\n",
        "                    misclassified_pred.append(int(pred_np))\n",
        "                    misclassified_target.append(int(target_np))\n",
        "                    misclassified_images.append(misclassified_imgs)\n",
        "                else:  # Multiple misclassifications\n",
        "                    for i in range(len(pred_np)):\n",
        "                        misclassified_pred.append(int(pred_np[i]))\n",
        "                        misclassified_target.append(int(target_np[i]))\n",
        "                        misclassified_images.append(misclassified_imgs[i])\n",
        "\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "    print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)')\n",
        "\n",
        "    # Plot first 25 misclassified images\n",
        "    if len(misclassified_images) > 0:\n",
        "        plt.figure(figsize=(10,10))\n",
        "        for i in range(min(25, len(misclassified_images))):\n",
        "            plt.subplot(5, 5, i+1)\n",
        "            plt.imshow(misclassified_images[i].squeeze(), cmap='gray')\n",
        "            plt.title(f'Pred: {misclassified_pred[i]}\\nTrue: {misclassified_target[i]}')\n",
        "            plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'misclassified_epoch.png')\n",
        "        plt.close()\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "hmT4c_iYe8uV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMWbLWO6FuHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3af12f64-32c2-4d56-d858-3f0416a69749"
      },
      "source": [
        "optimizer = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=0.001,\n",
        "    betas=(0.9, 0.999),\n",
        "    eps=1e-08,\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "\n",
        "# Modified scheduler for better convergence\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=0.004,              # Slightly higher max_lr\n",
        "    epochs=20,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    pct_start=0.2,             # Standard warmup\n",
        "    anneal_strategy='cos',\n",
        "    div_factor=10,\n",
        "    final_div_factor=100\n",
        ")\n",
        "\n",
        "best_acc = 0\n",
        "for epoch in range(1, 21):\n",
        "    print(f'Epoch {epoch}')\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    acc = test_with_misclassified(model, device, test_loader)\n",
        "    scheduler.step()\n",
        "\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        model = model.cpu()  # Move model to CPU before saving\n",
        "        torch.save(model, 'mnist_best.pth')\n",
        "        print(f'Best accuracy: {best_acc:.2f}%')\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.5289 Batch_id=384 Accuracy=78.76%: 100%|██████████| 385/385 [00:28<00:00, 13.49it/s]\n",
            "<ipython-input-5-673e9433ebf8>:53: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  misclassified_pred.append(int(pred_np[i]))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.4424, Accuracy: 9646/10000 (96.46%)\n",
            "Best accuracy: 96.46%\n",
            "Epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1680 Batch_id=384 Accuracy=96.72%: 100%|██████████| 385/385 [00:25<00:00, 15.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.1454, Accuracy: 9812/10000 (98.12%)\n",
            "Best accuracy: 98.12%\n",
            "Epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0822 Batch_id=384 Accuracy=97.96%: 100%|██████████| 385/385 [00:26<00:00, 14.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0809, Accuracy: 9872/10000 (98.72%)\n",
            "Best accuracy: 98.72%\n",
            "Epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0728 Batch_id=384 Accuracy=98.47%: 100%|██████████| 385/385 [00:26<00:00, 14.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0600, Accuracy: 9886/10000 (98.86%)\n",
            "Best accuracy: 98.86%\n",
            "Epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0951 Batch_id=384 Accuracy=98.68%: 100%|██████████| 385/385 [00:25<00:00, 14.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0478, Accuracy: 9892/10000 (98.92%)\n",
            "Best accuracy: 98.92%\n",
            "Epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0786 Batch_id=384 Accuracy=98.83%: 100%|██████████| 385/385 [00:25<00:00, 14.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0406, Accuracy: 9910/10000 (99.10%)\n",
            "Best accuracy: 99.10%\n",
            "Epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0628 Batch_id=384 Accuracy=98.94%: 100%|██████████| 385/385 [00:25<00:00, 14.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0354, Accuracy: 9911/10000 (99.11%)\n",
            "Best accuracy: 99.11%\n",
            "Epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0326 Batch_id=384 Accuracy=99.06%: 100%|██████████| 385/385 [00:26<00:00, 14.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0301, Accuracy: 9918/10000 (99.18%)\n",
            "Best accuracy: 99.18%\n",
            "Epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0499 Batch_id=384 Accuracy=99.14%: 100%|██████████| 385/385 [00:26<00:00, 14.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0289, Accuracy: 9919/10000 (99.19%)\n",
            "Best accuracy: 99.19%\n",
            "Epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0342 Batch_id=384 Accuracy=99.14%: 100%|██████████| 385/385 [00:26<00:00, 14.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0262, Accuracy: 9924/10000 (99.24%)\n",
            "Best accuracy: 99.24%\n",
            "Epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0139 Batch_id=384 Accuracy=99.19%: 100%|██████████| 385/385 [00:26<00:00, 14.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0278, Accuracy: 9919/10000 (99.19%)\n",
            "Epoch 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0289 Batch_id=384 Accuracy=99.29%: 100%|██████████| 385/385 [00:25<00:00, 14.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0221, Accuracy: 9934/10000 (99.34%)\n",
            "Best accuracy: 99.34%\n",
            "Epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0037 Batch_id=384 Accuracy=99.28%: 100%|██████████| 385/385 [00:26<00:00, 14.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0258, Accuracy: 9924/10000 (99.24%)\n",
            "Epoch 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0066 Batch_id=384 Accuracy=99.33%: 100%|██████████| 385/385 [00:26<00:00, 14.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0232, Accuracy: 9937/10000 (99.37%)\n",
            "Best accuracy: 99.37%\n",
            "Epoch 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0267 Batch_id=384 Accuracy=99.36%: 100%|██████████| 385/385 [00:26<00:00, 14.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0256, Accuracy: 9922/10000 (99.22%)\n",
            "Epoch 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0214 Batch_id=384 Accuracy=99.41%: 100%|██████████| 385/385 [00:26<00:00, 14.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0214, Accuracy: 9934/10000 (99.34%)\n",
            "Epoch 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0119 Batch_id=384 Accuracy=99.40%: 100%|██████████| 385/385 [00:25<00:00, 14.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0223, Accuracy: 9931/10000 (99.31%)\n",
            "Epoch 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0253 Batch_id=384 Accuracy=99.47%: 100%|██████████| 385/385 [00:25<00:00, 14.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0265, Accuracy: 9910/10000 (99.10%)\n",
            "Epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0377 Batch_id=384 Accuracy=99.42%: 100%|██████████| 385/385 [00:26<00:00, 14.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0221, Accuracy: 9935/10000 (99.35%)\n",
            "Epoch 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0070 Batch_id=384 Accuracy=99.48%: 100%|██████████| 385/385 [00:26<00:00, 14.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0188, Accuracy: 9946/10000 (99.46%)\n",
            "Best accuracy: 99.46%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2aJ0m3yZ685L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}