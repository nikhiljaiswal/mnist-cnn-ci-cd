{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m2JWFliFfKT"
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from torchvision import datasets, transforms\n",
        "from torchsummary import summary\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_Cx9q2QFgM7"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # Input Block - Enhanced feature extraction\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, 3, padding=1),    # 28x28x8\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.Conv2d(8, 8, 3, padding=1),    # 28x28x8\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.Conv2d(8, 16, 3, padding=1),   # 28x28x16\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(0.01)\n",
        "        )\n",
        "\n",
        "        # Transition Block 1\n",
        "        self.trans1 = nn.Sequential(\n",
        "            nn.MaxPool2d(2, 2),              # 14x14x16\n",
        "        )\n",
        "\n",
        "        # Convolution Block 2 - Focus on distinguishing similar digits\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 16, 3, padding=1),  # 14x14x16\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Conv2d(16, 32, 3, padding=1),  # 14x14x32\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(0.01)\n",
        "        )\n",
        "\n",
        "        # Transition Block 2\n",
        "        self.trans2 = nn.Sequential(\n",
        "            nn.MaxPool2d(2, 2),              # 7x7x32\n",
        "        )\n",
        "\n",
        "        # Convolution Block 3 - Final feature refinement\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(32, 32, 3, padding=1),  # 7x7x32\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, 16, 1),            # 7x7x16 (pointwise)\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(0.01)\n",
        "        )\n",
        "\n",
        "        self.gap = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=7)      # 1x1x16\n",
        "        )\n",
        "\n",
        "        self.final = nn.Sequential(\n",
        "            nn.Conv2d(16, 10, 1)             # 1x1x10\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.trans1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.trans2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.gap(x)\n",
        "        x = self.final(x)\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdydjYTZFyi3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7339c20-3742-4ab0-8fbd-1f93f3001bc4"
      },
      "source": [
        "model = Net().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 8, 28, 28]              80\n",
            "              ReLU-2            [-1, 8, 28, 28]               0\n",
            "       BatchNorm2d-3            [-1, 8, 28, 28]              16\n",
            "            Conv2d-4            [-1, 8, 28, 28]             584\n",
            "              ReLU-5            [-1, 8, 28, 28]               0\n",
            "       BatchNorm2d-6            [-1, 8, 28, 28]              16\n",
            "            Conv2d-7           [-1, 16, 28, 28]           1,168\n",
            "              ReLU-8           [-1, 16, 28, 28]               0\n",
            "       BatchNorm2d-9           [-1, 16, 28, 28]              32\n",
            "          Dropout-10           [-1, 16, 28, 28]               0\n",
            "        MaxPool2d-11           [-1, 16, 14, 14]               0\n",
            "           Conv2d-12           [-1, 16, 14, 14]           2,320\n",
            "             ReLU-13           [-1, 16, 14, 14]               0\n",
            "      BatchNorm2d-14           [-1, 16, 14, 14]              32\n",
            "           Conv2d-15           [-1, 32, 14, 14]           4,640\n",
            "             ReLU-16           [-1, 32, 14, 14]               0\n",
            "      BatchNorm2d-17           [-1, 32, 14, 14]              64\n",
            "          Dropout-18           [-1, 32, 14, 14]               0\n",
            "        MaxPool2d-19             [-1, 32, 7, 7]               0\n",
            "           Conv2d-20             [-1, 32, 7, 7]           9,248\n",
            "             ReLU-21             [-1, 32, 7, 7]               0\n",
            "      BatchNorm2d-22             [-1, 32, 7, 7]              64\n",
            "           Conv2d-23             [-1, 16, 7, 7]             528\n",
            "             ReLU-24             [-1, 16, 7, 7]               0\n",
            "      BatchNorm2d-25             [-1, 16, 7, 7]              32\n",
            "          Dropout-26             [-1, 16, 7, 7]               0\n",
            "        AvgPool2d-27             [-1, 16, 1, 1]               0\n",
            "           Conv2d-28             [-1, 10, 1, 1]             170\n",
            "================================================================\n",
            "Total params: 18,994\n",
            "Trainable params: 18,994\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.03\n",
            "Params size (MB): 0.07\n",
            "Estimated Total Size (MB): 1.10\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqTWLaM5GHgH"
      },
      "source": [
        "torch.manual_seed(1)\n",
        "batch_size = 128\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomRotation((-3.0, 3.0), fill=(1,)),  # Very conservative rotation\n",
        "    transforms.RandomAffine(\n",
        "        degrees=0,\n",
        "        translate=(0.05, 0.05),  # Reduced translation\n",
        "        scale=(0.98, 1.02),     # Minimal scaling\n",
        "        shear=(-2, 2),          # Minimal shear\n",
        "        fill=(1,)\n",
        "    ),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "kwargs = {'num_workers': 2, 'pin_memory': True} if use_cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                  transform=train_transforms),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=test_transforms),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    processed = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        processed += len(data)\n",
        "\n",
        "        pbar.set_description(desc=f'Loss={loss.item():0.4f} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}%')\n",
        "\n",
        "def test_with_misclassified(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    misclassified_images = []\n",
        "    misclassified_pred = []\n",
        "    misclassified_target = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "\n",
        "            # Store misclassified images\n",
        "            misclassified_mask = ~pred.eq(target.view_as(pred)).squeeze()\n",
        "            if misclassified_mask.any():\n",
        "                misclassified_imgs = data[misclassified_mask].cpu()\n",
        "                pred_np = pred[misclassified_mask].cpu().numpy()\n",
        "                target_np = target[misclassified_mask].cpu().numpy()\n",
        "\n",
        "                # Handle both single and multiple misclassifications\n",
        "                if len(misclassified_mask.size()) == 0:  # Single misclassification\n",
        "                    misclassified_pred.append(int(pred_np))\n",
        "                    misclassified_target.append(int(target_np))\n",
        "                    misclassified_images.append(misclassified_imgs)\n",
        "                else:  # Multiple misclassifications\n",
        "                    for i in range(len(pred_np)):\n",
        "                        misclassified_pred.append(int(pred_np[i]))\n",
        "                        misclassified_target.append(int(target_np[i]))\n",
        "                        misclassified_images.append(misclassified_imgs[i])\n",
        "\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "    print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)')\n",
        "\n",
        "    # Plot first 25 misclassified images\n",
        "    if len(misclassified_images) > 0:\n",
        "        plt.figure(figsize=(10,10))\n",
        "        for i in range(min(25, len(misclassified_images))):\n",
        "            plt.subplot(5, 5, i+1)\n",
        "            plt.imshow(misclassified_images[i].squeeze(), cmap='gray')\n",
        "            plt.title(f'Pred: {misclassified_pred[i]}\\nTrue: {misclassified_target[i]}')\n",
        "            plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'misclassified_epoch.png')\n",
        "        plt.close()\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "hmT4c_iYe8uV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMWbLWO6FuHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0306abbb-dc07-461f-d60a-20dd2baf2f23"
      },
      "source": [
        "optimizer = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=0.001,\n",
        "    betas=(0.9, 0.999),\n",
        "    eps=1e-08,\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "\n",
        "# Modified scheduler for better convergence\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=0.004,              # Slightly higher max_lr\n",
        "    epochs=20,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    pct_start=0.2,             # Standard warmup\n",
        "    anneal_strategy='cos',\n",
        "    div_factor=10,\n",
        "    final_div_factor=100\n",
        ")\n",
        "\n",
        "best_acc = 0\n",
        "for epoch in range(1, 21):\n",
        "    print(f'Epoch {epoch}')\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    acc = test_with_misclassified(model, device, test_loader)\n",
        "    scheduler.step()\n",
        "\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        torch.save(model.state_dict(), 'mnist_best.pth')\n",
        "        print(f'Best accuracy: {best_acc:.2f}%')\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.4596 Batch_id=468 Accuracy=78.50%: 100%|██████████| 469/469 [00:29<00:00, 15.64it/s]\n",
            "<ipython-input-5-673e9433ebf8>:53: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  misclassified_pred.append(int(pred_np[i]))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.3713, Accuracy: 9666/10000 (96.66%)\n",
            "Best accuracy: 96.66%\n",
            "Epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1580 Batch_id=468 Accuracy=97.13%: 100%|██████████| 469/469 [00:28<00:00, 16.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.1173, Accuracy: 9834/10000 (98.34%)\n",
            "Best accuracy: 98.34%\n",
            "Epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0649 Batch_id=468 Accuracy=98.21%: 100%|██████████| 469/469 [00:29<00:00, 15.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0742, Accuracy: 9868/10000 (98.68%)\n",
            "Best accuracy: 98.68%\n",
            "Epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0488 Batch_id=468 Accuracy=98.57%: 100%|██████████| 469/469 [00:28<00:00, 16.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0534, Accuracy: 9885/10000 (98.85%)\n",
            "Best accuracy: 98.85%\n",
            "Epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0717 Batch_id=468 Accuracy=98.81%: 100%|██████████| 469/469 [00:28<00:00, 16.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0414, Accuracy: 9911/10000 (99.11%)\n",
            "Best accuracy: 99.11%\n",
            "Epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0546 Batch_id=468 Accuracy=98.90%: 100%|██████████| 469/469 [00:28<00:00, 16.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0347, Accuracy: 9921/10000 (99.21%)\n",
            "Best accuracy: 99.21%\n",
            "Epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0351 Batch_id=468 Accuracy=99.03%: 100%|██████████| 469/469 [00:30<00:00, 15.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0352, Accuracy: 9892/10000 (98.92%)\n",
            "Epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0166 Batch_id=468 Accuracy=99.10%: 100%|██████████| 469/469 [00:28<00:00, 16.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0296, Accuracy: 9917/10000 (99.17%)\n",
            "Epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0735 Batch_id=468 Accuracy=99.15%: 100%|██████████| 469/469 [00:29<00:00, 16.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0284, Accuracy: 9911/10000 (99.11%)\n",
            "Epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0112 Batch_id=468 Accuracy=99.24%: 100%|██████████| 469/469 [00:28<00:00, 16.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0344, Accuracy: 9900/10000 (99.00%)\n",
            "Epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0222 Batch_id=468 Accuracy=99.30%: 100%|██████████| 469/469 [00:28<00:00, 16.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0327, Accuracy: 9910/10000 (99.10%)\n",
            "Epoch 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0117 Batch_id=468 Accuracy=99.31%: 100%|██████████| 469/469 [00:28<00:00, 16.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0226, Accuracy: 9930/10000 (99.30%)\n",
            "Best accuracy: 99.30%\n",
            "Epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0048 Batch_id=468 Accuracy=99.33%: 100%|██████████| 469/469 [00:28<00:00, 16.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0253, Accuracy: 9919/10000 (99.19%)\n",
            "Epoch 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0218 Batch_id=468 Accuracy=99.38%: 100%|██████████| 469/469 [00:28<00:00, 16.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0215, Accuracy: 9938/10000 (99.38%)\n",
            "Best accuracy: 99.38%\n",
            "Epoch 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0241 Batch_id=468 Accuracy=99.38%: 100%|██████████| 469/469 [00:28<00:00, 16.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0281, Accuracy: 9917/10000 (99.17%)\n",
            "Epoch 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0162 Batch_id=468 Accuracy=99.41%: 100%|██████████| 469/469 [00:29<00:00, 16.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0209, Accuracy: 9929/10000 (99.29%)\n",
            "Epoch 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0148 Batch_id=468 Accuracy=99.43%: 100%|██████████| 469/469 [00:28<00:00, 16.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0179, Accuracy: 9950/10000 (99.50%)\n",
            "Best accuracy: 99.50%\n",
            "Epoch 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0162 Batch_id=468 Accuracy=99.47%: 100%|██████████| 469/469 [00:28<00:00, 16.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0243, Accuracy: 9925/10000 (99.25%)\n",
            "Epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0425 Batch_id=468 Accuracy=99.48%: 100%|██████████| 469/469 [00:28<00:00, 16.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0211, Accuracy: 9929/10000 (99.29%)\n",
            "Epoch 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0208 Batch_id=468 Accuracy=99.50%: 100%|██████████| 469/469 [00:28<00:00, 16.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0231, Accuracy: 9918/10000 (99.18%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So5uk4EkHW6R"
      },
      "source": [],
      "execution_count": 6,
      "outputs": []
    }
  ]
}